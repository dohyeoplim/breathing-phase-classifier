import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader
from torch import nn, optim
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight

from src.dataset import BreathingAudioDataset
from src.model import Model
from src.utils.display import (
    print_start,
    print_epoch_summary,
    print_validation_accuracy,
    print_success,
    print_warning,
    print_error,
    progress_bar
)

# --- Ï†ÑÏó≠ ÏÑ§Ï†ï ---
IS_DEBUG_MODE = False 
INITIAL_PRINT_FEATURE_SUMMARY = True

# --- Ïû•Ïπò ÏÑ§Ï†ï ---
device = torch.device("mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu")
print(f"ÌòÑÏû¨ ÏÇ¨Ïö© Ï§ëÏù∏ Ïû•Ïπò: {device}")

PRINT_FEATURE_SUMMARY = INITIAL_PRINT_FEATURE_SUMMARY

def evaluate_model(model, validation_data_loader, epoch_index_for_debug=None):
    model.eval()
    total_correct = 0
    total_samples = 0
    predicted_probabilities = []
    all_labels = []
    global PRINT_FEATURE_SUMMARY

    with torch.no_grad():
        for batch_idx, (features, labels) in enumerate(validation_data_loader):
            features, labels = features.to(device), labels.to(device)

            if PRINT_FEATURE_SUMMARY and epoch_index_for_debug == 1 and batch_idx == 0:
                print("\n--- Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ Ï≤´ Î∞∞Ïπò ÌäπÏßï ÏöîÏïΩ ---")
                print(f"Feature tensor shape: {features.shape}")
                if features.numel() > 0:
                    print(f"Feature tensor mean: {features.mean().item():.4f}, std: {features.std().item():.4f}")
                    print(f"Feature tensor min: {features.min().item():.4f}, max: {features.max().item():.4f}")
                else:
                    print("Feature tensor is empty.")
                print(f"Labels in batch (first 10): {labels.cpu().numpy().flatten()[:10]}")
                print("-------------------------------------\n")

            outputs = model(features)
            probabilities = torch.sigmoid(outputs)
            predictions = (probabilities > 0.5).int().squeeze()

            if predictions.dim() == 0: predictions = predictions.unsqueeze(0)
            if labels.dim() == 0: labels = labels.unsqueeze(0)
            
            current_probs_list = []
            if probabilities.numel() > 0 :
                if probabilities.dim() == 0: 
                    current_probs_list.append(probabilities.item())
                else: 
                    squeezed_probs = probabilities.cpu().squeeze()
                    if squeezed_probs.dim() == 0: 
                        current_probs_list.append(squeezed_probs.item())
                    elif squeezed_probs.numel() > 0: 
                         current_probs_list.extend(squeezed_probs.tolist())
            
            if current_probs_list: predicted_probabilities.extend(current_probs_list)

            all_labels.extend(labels.cpu().tolist())
            total_correct += (predictions == labels).sum().item()
            total_samples += labels.size(0)

    accuracy = total_correct / total_samples if total_samples > 0 else 0.0
    min_prob = min(predicted_probabilities) if predicted_probabilities else 0.0
    max_prob = max(predicted_probabilities) if predicted_probabilities else 0.0
    return accuracy, predicted_probabilities, all_labels


def train_model():
    print_start("Training")
    global PRINT_FEATURE_SUMMARY
    PRINT_FEATURE_SUMMARY = INITIAL_PRINT_FEATURE_SUMMARY 

    dataframe = pd.read_csv("input/train.csv")

    train_dataframe, validation_dataframe = train_test_split(
        dataframe, test_size=0.2, stratify=dataframe["Target"], random_state=42
    )
    print(f"ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Ïàò: {len(train_dataframe)}, Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ Ïàò: {len(validation_dataframe)}")

    numeric_labels_train = train_dataframe["Target"].map(lambda label: 1 if label == "E" else 0)
    if not numeric_labels_train.empty:
        unique_labels_in_train, counts_in_train = np.unique(numeric_labels_train, return_counts=True)
        print(f"ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨: {dict(zip(unique_labels_in_train, counts_in_train))}")
        if len(unique_labels_in_train) == 2:
            class_weights = compute_class_weight(class_weight="balanced", classes=unique_labels_in_train, y=numeric_labels_train)
            positive_class_index = np.where(unique_labels_in_train == 1)[0]
            positive_class_weight = torch.tensor(class_weights[positive_class_index[0]] if len(positive_class_index) > 0 else 1.0, dtype=torch.float).to(device)
        else:
            print_warning(f"ÌïôÏäµ Îç∞Ïù¥ÌÑ∞Ïóê Îã®Ïùº ÌÅ¥ÎûòÏä§Îßå Ï°¥Ïû¨ÌïòÍ±∞ÎÇò({unique_labels_in_train}) Î†àÏù¥Î∏îÏù¥ ÎπÑÏñ¥ÏûàÏäµÎãàÎã§. ÌÅ¥ÎûòÏä§ Í∞ÄÏ§ëÏπòÎäî 1.0ÏúºÎ°ú ÏÑ§Ï†ïÎê©ÎãàÎã§.")
            positive_class_weight = torch.tensor(1.0, dtype=torch.float).to(device)
        print(f"Í≥ÑÏÇ∞Îêú Positive ÌÅ¥ÎûòÏä§ Í∞ÄÏ§ëÏπò (1Ïóê ÎåÄÌïú Í∞ÄÏ§ëÏπò): {positive_class_weight.item():.4f}")
    else:
        print_error("ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ÏÖãÏóê Î†àÏù¥Î∏îÏù¥ ÏóÜÏäµÎãàÎã§! Îç∞Ïù¥ÌÑ∞ Î°úÎî©ÏùÑ ÌôïÏù∏ÌïòÏÑ∏Ïöî.")
        return

    # --- Dataset ÌååÎùºÎØ∏ÌÑ∞ ---
    SAMPLING_RATE = 16000
    DURATION_IN_SECONDS = 2
    N_MELS = 128
    N_MFCC = 40
    HOP_LENGTH = 512
    N_FFT = 2048
    N_MELS_FOR_ENTROPY = 128
    N_FFT_FOR_ENTROPY = 2048
    HOP_LENGTH_FOR_ENTROPY = 512

    train_dataset = BreathingAudioDataset(train_dataframe, "input/train", is_training=True,
                                          sampling_rate=SAMPLING_RATE, duration_in_seconds=DURATION_IN_SECONDS,
                                          n_mels=N_MELS, n_mfcc=N_MFCC, hop_length=HOP_LENGTH, n_fft=N_FFT,
                                          n_mels_for_entropy=N_MELS_FOR_ENTROPY, n_fft_for_entropy=N_FFT_FOR_ENTROPY, hop_length_for_entropy=HOP_LENGTH_FOR_ENTROPY)
    validation_dataset = BreathingAudioDataset(validation_dataframe, "input/train", is_training=True,
                                               sampling_rate=SAMPLING_RATE, duration_in_seconds=DURATION_IN_SECONDS,
                                               n_mels=N_MELS, n_mfcc=N_MFCC, hop_length=HOP_LENGTH, n_fft=N_FFT,
                                               n_mels_for_entropy=N_MELS_FOR_ENTROPY, n_fft_for_entropy=N_FFT_FOR_ENTROPY, hop_length_for_entropy=HOP_LENGTH_FOR_ENTROPY)

    # --- ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï ---
    LEARNING_RATE = 1e-4  
    BATCH_SIZE = 32       
    WEIGHT_DECAY = 3e-4   # Í∑úÏ†ú Í∞ïÌôî (Ïù¥Ï†Ñ 2e-4 -> 3e-4)
    GRU_HIDDEN_SIZE = 128 
    NUM_GRU_LAYERS = 2
    DROPOUT_RATE = 0.4    # Dropout Í∞ïÌôî (Ïù¥Ï†Ñ 0.35 -> 0.4)
    NUM_EPOCHS = 300      
    EARLY_STOPPING_PATIENCE = 25 

    train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=False)
    validation_data_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, num_workers=0, pin_memory=False)

    ACTUAL_NUM_INPUT_FEATURES = N_MELS + N_MFCC + 9 
    model = Model(num_freq_bins=ACTUAL_NUM_INPUT_FEATURES,
                  gru_hidden_size=GRU_HIDDEN_SIZE,
                  num_gru_layers=NUM_GRU_LAYERS,
                  dropout_rate=DROPOUT_RATE).to(device)
    print(f"\n--- Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò ---")
    print(f"ÏûÖÎ†• ÌäπÏßï Ïàò (num_freq_bins): {ACTUAL_NUM_INPUT_FEATURES}")
    print(model)
    print(f"Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞ Ïàò: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    print("---------------------\n")

    loss_function = nn.BCEWithLogitsLoss(pos_weight=positive_class_weight).to(device)
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    # Ïä§ÏºÄÏ§ÑÎü¨ factorÎ•º ÏïΩÍ∞Ñ ÎÜíÏó¨ ÌïôÏäµÎ•† Í∞êÏÜå Ìè≠ ÏôÑÌôî, patienceÎäî Ïú†ÏßÄ ÎòêÎäî ÏïΩÍ∞Ñ Ï°∞Ï†ï
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode="max", factor=0.5, patience=10, min_lr=1e-7, verbose=False) 


    os.makedirs("models", exist_ok=True)
    best_validation_accuracy = 0.0
    best_epoch = 0
    early_stopping_counter = 0

    print_warning(f"ÌïôÏäµ ÏãúÏûë: LR={LEARNING_RATE}, Batch={BATCH_SIZE}, WeightDecay={WEIGHT_DECAY}, GRU_Hidden={GRU_HIDDEN_SIZE}, GRU_Layers={NUM_GRU_LAYERS}, Dropout={DROPOUT_RATE}, Epochs={NUM_EPOCHS}, EarlyStopPatience={EARLY_STOPPING_PATIENCE}")

    for epoch_index in range(1, NUM_EPOCHS + 1):
        model.train()
        total_loss = 0.0
        progress = progress_bar(train_data_loader, description=f"üì¶ Epoch {epoch_index}/{NUM_EPOCHS}")

        for batch_idx, (features, labels) in enumerate(progress):
            features, labels = features.to(device), labels.float().unsqueeze(1).to(device)

            if PRINT_FEATURE_SUMMARY and epoch_index == 1 and batch_idx == 0:
                print("\n--- ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Ï≤´ Î∞∞Ïπò ÌäπÏßï ÏöîÏïΩ ---")
                if features.numel() > 0:
                    print(f"Feature tensor shape: {features.shape}")
                    print(f"Feature tensor mean: {features.mean().item():.4f}, std: {features.std().item():.4f}")
                    print(f"Feature tensor min: {features.min().item():.4f}, max: {features.max().item():.4f}")
                else:
                    print("Feature tensor is empty.")
                print(f"Labels in batch (first 10): {labels.cpu().numpy().flatten()[:10]}")
                print("-------------------------------------\n")
                PRINT_FEATURE_SUMMARY = False

            optimizer.zero_grad()
            outputs = model(features)
            loss = loss_function(outputs, labels)

            if torch.isnan(loss) or torch.isinf(loss):
                print_error(f"ÏÜêÏã§ Í∞í Ïò§Î•ò (NaN ÎòêÎäî Inf) Î∞úÏÉù! ÏóêÌè¨ÌÅ¨ {epoch_index}, Î∞∞Ïπò {batch_idx}. ÌïôÏäµÏùÑ Ï§ëÎã®Ìï©ÎãàÎã§.")
                if features.numel() > 0: print_error(f"ÌäπÏßïÍ∞í (features) mean: {features.mean().item()}, std: {features.std().item()}")
                if outputs.numel() > 0: print_error(f"Ï∂úÎ†•Í∞í (outputs) mean: {outputs.mean().item()}, std: {outputs.std().item()}")
                return

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            total_loss += loss.item()
            progress.set_postfix({"Loss": f"{loss.item():.4f}", "LR": f"{optimizer.param_groups[0]['lr']:.1e}"})

        average_loss = total_loss / len(train_data_loader) if len(train_data_loader) > 0 else 0.0
        print_epoch_summary(epoch_index, average_loss)

        validation_accuracy, val_probs, _ = evaluate_model(model, validation_data_loader, epoch_index_for_debug=epoch_index)
        min_p = min(val_probs) if val_probs else 0.0
        max_p = max(val_probs) if val_probs else 0.0
        print_validation_accuracy(validation_accuracy, min_p, max_p)

        prev_lr = optimizer.param_groups[0]['lr']
        scheduler.step(validation_accuracy)
        current_lr = optimizer.param_groups[0]['lr']
        if abs(prev_lr - current_lr) > 1e-9: 
            print_warning(f"ÌïôÏäµÎ•† Î≥ÄÍ≤ΩÎê®: {prev_lr:.2e} -> {current_lr:.2e} (ÏóêÌè¨ÌÅ¨ {epoch_index}, Val Acc: {validation_accuracy:.4f})")
        else:
            print(f"ÏóêÌè¨ÌÅ¨ {epoch_index} Ï¢ÖÎ£å. ÌòÑÏû¨ ÌïôÏäµÎ•†: {current_lr:.2e}")


        if validation_accuracy > best_validation_accuracy + 1e-5 : # ÏïÑÏ£º ÎØ∏ÎØ∏Ìïú Í∞úÏÑ†(1e-5 Ïù¥ÏÉÅ)ÎèÑ Ïù∏Ï†ï
            print_success(f"ÏÑ±Îä• Í∞úÏÑ†! Ïù¥Ï†Ñ ÏµúÍ≥†: {best_validation_accuracy:.4f} -> ÌòÑÏû¨: {validation_accuracy:.4f}")
            best_validation_accuracy = validation_accuracy
            best_epoch = epoch_index
            torch.save(model.state_dict(), "models/best_model.pth")
            print_success(f"üöÄ Best model saved at Epoch {best_epoch} with accuracy: {best_validation_accuracy:.4f} (LR: {current_lr:.2e})")
            early_stopping_counter = 0
        else:
            early_stopping_counter += 1
            print_warning(f"ÏÑ±Îä• Í∞úÏÑ† ÏóÜÏùå (ÎòêÎäî ÎØ∏ÎØ∏Ìï®). Early stopping counter: {early_stopping_counter}/{EARLY_STOPPING_PATIENCE}. Best Acc: {best_validation_accuracy:.4f} at Epoch {best_epoch}")
            if early_stopping_counter >= EARLY_STOPPING_PATIENCE:
                print_warning(f"‚èπÔ∏è Early stopping at epoch {epoch_index}. Best Validation Accuracy: {best_validation_accuracy:.4f} (achieved at Epoch {best_epoch})")
                break

        if epoch_index % 50 == 0 and not IS_DEBUG_MODE: 
             print_warning(f"üíæ Saving checkpoint model at epoch {epoch_index}...")
             torch.save(model.state_dict(), f"models/model_epoch{epoch_index}.pth")

    print_success(f"üéâ Training completed. Best Validation Accuracy: {best_validation_accuracy:.4f} (achieved at Epoch {best_epoch})")

# if __name__ == '__main__':
#     train_model()
#     pass